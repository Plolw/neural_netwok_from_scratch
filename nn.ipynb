{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69cc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    data = np.array()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b9fe121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_shopping(directory):\n",
    "    with open(directory) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        evidence = []\n",
    "        labels = []\n",
    "\n",
    "        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'June', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "        for row in reader:\n",
    "            row_evidence = [\n",
    "                int(row[\"Administrative\"]),\n",
    "                float(row[\"Administrative_Duration\"]),\n",
    "                int(row[\"Informational\"]),\n",
    "                float(row[\"Informational_Duration\"]),\n",
    "                int(row[\"ProductRelated\"]),\n",
    "                float(row[\"ProductRelated_Duration\"]),\n",
    "                float(row[\"BounceRates\"]),\n",
    "                float(row[\"ExitRates\"]),\n",
    "                float(row[\"PageValues\"]),\n",
    "                float(row[\"SpecialDay\"]),\n",
    "                months.index(row[\"Month\"]),\n",
    "                int(row[\"OperatingSystems\"]),\n",
    "                int(row[\"Browser\"]),\n",
    "                int(row[\"Region\"]),\n",
    "                int(row[\"TrafficType\"]),\n",
    "                1 if row[\"VisitorType\"] == \"Returning_Visitor\" else 0,\n",
    "                1 if row[\"Weekend\"] == \"TRUE\" else 0\n",
    "            ]\n",
    "\n",
    "            label = 1 if row[\"Revenue\"] == \"TRUE\" else 0\n",
    "\n",
    "            evidence.append(row_evidence)\n",
    "            labels.append(label)\n",
    "\n",
    "        return (evidence, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb935145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(layers: int, n_neurons: list):\n",
    "    if len(n_neurons) != layers:\n",
    "        return \"Invalid number of neurons per layer.\"\n",
    "    weights = []\n",
    "    bias = []\n",
    "    for i in range(layers - 1):\n",
    "        W = np.random.randn(n_neurons[i + 1], n_neurons[i])\n",
    "        b = np.random.randn(n_neurons[i + 1], 1)\n",
    "        weights.append(W)\n",
    "        bias.append(b)\n",
    "    return (weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5aa3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_ACTIVATION_FUNCTIONS = (\"ReLU\", \"Sigmoid\")\n",
    "SUPPORTED_LOSS_FUNCTIONS = (\"MSE\", \"Cross-entropy\")\n",
    "\n",
    "def forward(X, weights, bias, functions):\n",
    "    if len(functions) != len(weights):\n",
    "        return \"Invalid number of functions.\"\n",
    "    A = []\n",
    "    Z = []\n",
    "    a = X\n",
    "    for i in range(len(weights)):\n",
    "        z = a.dot(weights[i].T) + bias[i].T\n",
    "        Z.append(z)\n",
    "        f = functions[i]\n",
    "        if f not in SUPPORTED_ACTIVATION_FUNCTIONS:\n",
    "            return \"Function type '\" + f + \"' not supported.\"\n",
    "        if f == \"ReLU\":\n",
    "            a = ReLU(z)\n",
    "        elif f == \"Sigmoid\":\n",
    "            a = Sigmoid(z)\n",
    "        elif f == \"Softmax\":\n",
    "            a = Softmax(z)\n",
    "        else:\n",
    "            return \"Specify activation function.\"\n",
    "        A.append(a)\n",
    "    return A, Z\n",
    "\n",
    "def backward(A, W, Z, y, functions, loss):\n",
    "    n_layers = len(A)\n",
    "    gradients_w = []\n",
    "    gradients_b = []\n",
    "    for i in range(n_layers):\n",
    "        print(n_layers)\n",
    "        if functions[n_layers - i - 1] not in SUPPORTED_ACTIVATION_FUNCTIONS:\n",
    "            return \"Function type '\" + functions[n_layers - i - 1] + \"' not supported.\"\n",
    "        if functions[n_layers - i - 1] == \"ReLU\":\n",
    "            dadz = ReLU_derivative(Z[n_layers - i - 1])\n",
    "        elif functions[n_layers - i - 1] == \"Sigmoid\":\n",
    "            dadz = Sigmoid_derivative(Z[n_layers - i - 1])\n",
    "        else:\n",
    "            return \"Invalid activation function type.\"\n",
    "        if loss not in SUPPORTED_LOSS_FUNCTIONS:\n",
    "            return \"Loss function type '\" + loss + \"' not supported.\"\n",
    "        if i == 0:\n",
    "            if loss == \"MSE\":\n",
    "                djda = MSE_derivative(A[n_layers - i - 1], y)\n",
    "            elif loss == \"Cross-entropy\":\n",
    "                djda = Cross_entropy_derivative(A[n_layers - i - 1], y)\n",
    "            dz = djda * dadz\n",
    "        else:\n",
    "            dz = dz.dot(W[n_layers - i - 1].T) * dadz\n",
    "        dw = A[n_layers - i - 2].T.dot(dz)\n",
    "        db = np.mean(dz, axis=0, keepdims=True).T\n",
    "        gradients_w.insert(0,dw)\n",
    "        gradients_b.insert(0,db)\n",
    "    return gradients_w, gradients_b\n",
    "\n",
    "def Sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def Sigmoid_derivative(z):\n",
    "    s = Sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def ReLU_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def Softmax(z):\n",
    "    ez = np.exp(z - np.max(z))\n",
    "    return ez / np.sum(ez)\n",
    "\n",
    "def Softmax_derivative(z):\n",
    "    S = Softmax(z)\n",
    "    return S * (1 - S)\n",
    "\n",
    "\n",
    "def MSE_derivative(a, y):\n",
    "    return (a - y) / len(y)\n",
    "\n",
    "def Cross_entropy_derivative(a, y):\n",
    "    return a - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "391e4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W, b, grads_w, grads_b, alpha):\n",
    "    for i in range(len(W)):\n",
    "        W[i] = W[i] - alpha*grads_w[i].T\n",
    "        b[i] = b[i] - alpha*grads_b[i].T\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38298a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(epochs)\n\u001b[1;32m---> 88\u001b[0m     A, Z \u001b[38;5;241m=\u001b[39m forward(X, weights, bias, functions)\n\u001b[0;32m     89\u001b[0m     gradients_w, gradients_b \u001b[38;5;241m=\u001b[39m backward(A, weights, Z, y, functions, loss)\n\u001b[0;32m     90\u001b[0m     weights, bias \u001b[38;5;241m=\u001b[39m update_params(weights, bias, gradients_w, gradients_b, alpha)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "\n",
    "def backward(A, W, Z, y, functions, loss):\n",
    "    n_layers = len(W)\n",
    "    gradients_w = []\n",
    "    gradients_b = []\n",
    "    \n",
    "    # Start from output layer and move backwards\n",
    "    for i in reversed(range(n_layers)):\n",
    "        if i == n_layers - 1:  # Output layer\n",
    "            if loss == \"MSE\":\n",
    "                djda = MSE_derivative(A[-1], y)\n",
    "            elif loss == \"Cross-entropy\":\n",
    "                djda = Cross_entropy_derivative(A[-1], y)\n",
    "            else:\n",
    "                return \"Invalid loss function\"\n",
    "            \n",
    "            if functions[-1] == \"ReLU\":\n",
    "                dadz = ReLU_derivative(Z[-1])\n",
    "            elif functions[-1] == \"Sigmoid\":\n",
    "                dadz = Sigmoid_derivative(Z[-1])\n",
    "            elif functions[-1] == \"Softmax\":\n",
    "                dadz = Softmax_derivative(Z[-1])\n",
    "            else:\n",
    "                return \"Invalid activation function\"\n",
    "            \n",
    "            dz = djda * dadz\n",
    "        else:  # Hidden layers\n",
    "            if functions[i] == \"ReLU\":\n",
    "                dadz = ReLU_derivative(Z[i])\n",
    "            elif functions[i] == \"Sigmoid\":\n",
    "                dadz = Sigmoid_derivative(Z[i])\n",
    "            elif functions[i] == \"Softmax\":\n",
    "                dadz = Softmax_derivative(Z[i])\n",
    "            else:\n",
    "                return \"Invalid activation function\"\n",
    "            \n",
    "            dz = dz.dot(W[i+1]) * dadz\n",
    "        \n",
    "        # Calculate gradients\n",
    "        if i == 0:\n",
    "            a_prev = X\n",
    "        else:\n",
    "            a_prev = A[i-1]\n",
    "            \n",
    "        dw = a_prev.T.dot(dz) / len(y)\n",
    "        db = np.mean(dz, axis=0, keepdims=True)\n",
    "        \n",
    "        gradients_w.insert(0, dw)\n",
    "        gradients_b.insert(0, db)\n",
    "    \n",
    "    return gradients_w, gradients_b\n",
    "\n",
    "\"\"\" evidence, labels = load_data_shopping(\"shopping.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    evidence, labels, test_size=0.3\n",
    ")\n",
    "\n",
    "y = np.eye(2)[y_train]\n",
    "X = np.array(X_train) \"\"\"\n",
    "\n",
    "X = np.array([\n",
    "    [0.5, 1.2, 0.8],\n",
    "    [1.5, 0.3, 1.0]\n",
    "])\n",
    "\n",
    "y = np.eye(2)[np.array([0, 1])]\n",
    "\n",
    "#define network\n",
    "alpha = 0.1\n",
    "epochs = 50\n",
    "layers = 3\n",
    "n_neurons = [len(X[0]), 3, 2]\n",
    "functions = [\"ReLU\", \"Softmax\"]\n",
    "loss = \"MSE\"\n",
    "\n",
    "weights, bias = initialize_params(layers, n_neurons)\n",
    "\n",
    "#A, Z = forward(X, weights, bias, functions)\n",
    "\n",
    "# gradients_w, gradients_b = backward(A, weights, Z, y, functions, loss)\n",
    "\n",
    "# W, b = update_params(weights, bias, gradients_w, gradients_b, alpha)\n",
    "\n",
    "\n",
    "while epochs > 0:\n",
    "    print(epochs)\n",
    "    A, Z = forward(X, weights, bias, functions)\n",
    "    gradients_w, gradients_b = backward(A, weights, Z, y, functions, loss)\n",
    "    weights, bias = update_params(weights, bias, gradients_w, gradients_b, alpha)\n",
    "    epochs -= 1\n",
    "\n",
    "A, Z = forward(X, weights, bias, functions)\n",
    "print(A[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
